{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "## It's better to ensure the current path\n",
    "os.chdir('../doc')\n",
    "sys.path.append('../lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SFP91yyychql"
   },
   "source": [
    "# Getting Started:\n",
    "## A simple driving model training and evaluation pipeline using the Drive360 dataset and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eeTTXKFEchqm"
   },
   "source": [
    "## Loading data from Drive360 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dUjc8BHchqn"
   },
   "source": [
    "The **dataset.py** file contains the 3 classes necessary for creating a Drive360Loader. Using the **config.json** file to specify the location of the csv and data directory, we can generate phase (train, validation, test) specific data loaders that can output samples from each set. Adjust the **dataset.py** to your preferred training framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5075,
     "status": "ok",
     "timestamp": 1569969319418,
     "user": {
      "displayName": "Xiaoxi Zhao",
      "photoUrl": "",
      "userId": "07703831001558590239"
     },
     "user_tz": 240
    },
    "id": "FMS1e2vVchqo",
    "outputId": "3c6f4b5c-097e-4de6-cc7e-84a152d06319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train # of data: 35591\n",
      "Phase: validation # of data: 2342\n",
      "Phase: test # of data: 6173\n",
      "Loaded train loader with the following data available as a dict.\n",
      "Index(['cameraRight', 'cameraFront', 'cameraRear', 'cameraLeft', 'canSteering',\n",
      "       'canSpeed', 'chapter'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dataset import Drive360Loader\n",
    "\n",
    "# load the config.json file that specifies data \n",
    "# location parameters and other hyperparameters \n",
    "# required.\n",
    "config = json.load(open('./config.json'))\n",
    "\n",
    "normalize_targets = config['target']['normalize']\n",
    "target_mean = config['target']['mean']\n",
    "target_std = config['target']['std']\n",
    "\n",
    "# create a train, validation and test data loader\n",
    "train_loader = Drive360Loader(config, 'train')\n",
    "validation_loader = Drive360Loader(config, 'validation')\n",
    "test_loader = Drive360Loader(config, 'test')\n",
    "\n",
    "# print the data (keys) available for use. See full \n",
    "# description of each data type in the documents.\n",
    "print('Loaded train loader with the following data available as a dict.')\n",
    "print(train_loader.drive360.dataframe.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMV2svdJchqt"
   },
   "source": [
    "## Training a basic driving model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bvQMkdW0chqu"
   },
   "source": [
    "Create your driving model. This is specific to your learning framework. \n",
    "\n",
    "Below we give a very basic dummy model that uses the front facing camera and a resnet34 + LSTM architecture to predict canSteering and canSpeed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Non_local_pytorch.lib.non_local_embedded_gaussian import NONLocalBlock2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8439,
     "status": "ok",
     "timestamp": 1569969364254,
     "user": {
      "displayName": "Xiaoxi Zhao",
      "photoUrl": "",
      "userId": "07703831001558590239"
     },
     "user_tz": 240
    },
    "id": "QQNxTxMOchqv",
    "outputId": "26246366-3f5a-4930-bf2f-64433676363c"
   },
   "outputs": [],
   "source": [
    "class NonLocalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NonLocalModel, self).__init__()\n",
    "        final_concat_size = 0\n",
    "        \n",
    "        # Main CNN\n",
    "        cnn = models.resnet34(pretrained=True)\n",
    "        for i, layer in enumerate(cnn.children()):\n",
    "            if i <= 6:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "        self.features = nn.Sequential(\n",
    "            *list(cnn.children())[0:4],\n",
    "            NONLocalBlock2D(64),\n",
    "            list(cnn.children())[4],\n",
    "            NONLocalBlock2D(64),\n",
    "            list(cnn.children())[5],\n",
    "            NONLocalBlock2D(128),\n",
    "            list(cnn.children())[6],\n",
    "            NONLocalBlock2D(256),\n",
    "            *list(cnn.children())[7:9]\n",
    "        )\n",
    "        # self.resnet_output = nn.Sequential(*list(cnn.children())[:-2])\n",
    "        self.intermediate = nn.Sequential(nn.Linear(\n",
    "                          cnn.fc.in_features, 128),\n",
    "                          nn.ReLU())\n",
    "        final_concat_size += 128\n",
    "\n",
    "        # Main LSTM\n",
    "        self.gru = nn.GRU(input_size=128,\n",
    "                            hidden_size=64,\n",
    "                            num_layers=3,\n",
    "                            batch_first=False)\n",
    "        final_concat_size += 64\n",
    "        \n",
    "        # Angle Regressor\n",
    "        self.control_angle = nn.Sequential(\n",
    "            nn.Linear(final_concat_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        # Speed Regressor\n",
    "        self.control_speed = nn.Sequential(\n",
    "            nn.Linear(final_concat_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        module_outputs = []\n",
    "        gru_i = []\n",
    "        # Loop through temporal sequence of\n",
    "        # front facing camera images and pass \n",
    "        # through the cnn.\n",
    "        for k, v in data['cameraFront'].items():\n",
    "            if torch.cuda.is_available():\n",
    "                v = v.cuda()\n",
    "            x = self.features(v)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.intermediate(x)\n",
    "            gru_i.append(x)\n",
    "            # feed the current front facing camera\n",
    "            # output directly into the \n",
    "            # regression networks.\n",
    "            if k == 0:\n",
    "                module_outputs.append(x)\n",
    "\n",
    "        # Feed temporal outputs of CNN into LSTM\n",
    "        self.gru.flatten_parameters()\n",
    "        i_gru, _ = self.gru(torch.stack(gru_i))\n",
    "        module_outputs.append(i_gru[-1])\n",
    "        \n",
    "        # Concatenate current image CNN output \n",
    "        # and LSTM output.\n",
    "        x_cat = torch.cat(module_outputs, dim=-1)\n",
    "        \n",
    "        # Feed concatenated outputs into the \n",
    "        # regession networks.\n",
    "        prediction = {'canSteering': torch.squeeze(self.control_angle(x_cat)),\n",
    "                      'canSpeed': torch.squeeze(self.control_speed(x_cat))}\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5EEBPLHnchqy"
   },
   "source": [
    "### Training and validation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2662274,
     "status": "ok",
     "timestamp": 1569977854128,
     "user": {
      "displayName": "Xiaoxi Zhao",
      "photoUrl": "",
      "userId": "07703831001558590239"
     },
     "user_tz": 240
    },
    "id": "3WvoNo2Ychqz",
    "outputId": "d0093f0a-0ebb-4c58-fd5d-8d0ad3c93a7c"
   },
   "outputs": [],
   "source": [
    "def train_nn(train_loader, validation_loader, model, optimizer, \n",
    "             criterion, epochs=5, validation=True, load_path='', save_path='', print_freq = 200):\n",
    "    '''Training the model\n",
    "    Args:\n",
    "        validation: boolean, whether process validation\n",
    "        load_path: string, the model weights file to load\n",
    "        save_path: string, the model weights file to save\n",
    "    Returns:\n",
    "    '''\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        # model = nn.DataParallel(model)\n",
    "    if os.path.exists(load_path):\n",
    "        print('='*10 + 'loading weights from ' + load_path + '='*10)\n",
    "        checkpoint = torch.load(load_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print('='*10 + 'loading finished' + '='*10)\n",
    "    else:\n",
    "        print('load_path does not exist!')\n",
    "        \n",
    "    train_metrics = {\n",
    "        'angle_loss': {},\n",
    "        'speed_loss': {}\n",
    "    }\n",
    "    validation_metrics = {\n",
    "        'angle_loss': {},\n",
    "        'speed_loss': {}\n",
    "    }\n",
    "    for epoch in range(epochs):\n",
    "        print('='*10 + 'start ' + str(epoch+1) + ' epoch' + '='*10)\n",
    "        model.train()\n",
    "        angle_loss = 0.0\n",
    "        speed_loss = 0.0\n",
    "        since = datetime.now()\n",
    "        cnt = 0\n",
    "        for batch_idx, (data, target) in enumerate(tqdm_notebook(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(data)\n",
    "            loss1 = criterion(prediction['canSteering'].cpu(), target['canSteering'].cpu())\n",
    "            loss2 = criterion(prediction['canSpeed'].cpu(), target['canSpeed'].cpu())\n",
    "            loss = loss1 + loss2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print statistics\n",
    "            angle_loss += loss1.item()\n",
    "            speed_loss += loss2.item()\n",
    "            cnt += 1\n",
    "            if batch_idx > 5:\n",
    "                break\n",
    "            if (batch_idx+1) % print_freq == 0:\n",
    "                if normalize_targets:\n",
    "                    angle_loss = (angle_loss * target_std['canSteering']**2) / cnt\n",
    "                    speed_loss = (speed_loss * target_std['canSpeed']**2) / cnt\n",
    "                else:\n",
    "                    angle_loss /= cnt\n",
    "                    speed_loss /= cnt\n",
    "                train_metrics['angle_loss'].setdefault(str(epoch), []).append(angle_loss)\n",
    "                train_metrics['speed_loss'].setdefault(str(epoch), []).append(speed_loss)\n",
    "                print('[epoch: %d, batch: %5d] time: %.2f angle_loss: %.2f speed_loss: %.2f' %\n",
    "                      (epoch + 1, batch_idx + 1, (datetime.now() - since).total_seconds(), angle_loss, speed_loss))\n",
    "                angle_loss = 0.0\n",
    "                speed_loss = 0.0\n",
    "                since = datetime.now()\n",
    "                cnt = 0\n",
    "        print('='*10 + 'saving the model to' + save_path + '='*10)\n",
    "        torch.save({\n",
    "            \"model_state_dict\":model.state_dict(),\n",
    "            \"angle_loss\": angle_loss,\n",
    "            \"speed_loss\": speed_loss,\n",
    "            \"optimizer_state_dict\":optimizer.state_dict(),\n",
    "            \"epoch\":epoch\n",
    "            }, save_path)\n",
    "        print('saving success!')\n",
    "        if validation:\n",
    "            print('='*10 + 'starting validation' + '='*10)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, target) in enumerate(tqdm_notebook(validation_loader)):\n",
    "                    if torch.cuda.is_available():\n",
    "                        for w in ['canSteering', 'canSpeed']:\n",
    "                            target[w].cuda()\n",
    "                    prediction = model(data)\n",
    "                    mse1 = (np.square(prediction['canSteering'].cpu() - target['canSteering'].cpu())).mean()\n",
    "                    mse2 = (np.square(prediction['canSpeed'].cpu() - target['canSpeed'].cpu())).mean()\n",
    "                    if normalize_targets:\n",
    "                        mse1 = mse1 * target_std['canSteering'] ** 2\n",
    "                        mse2 = mse2 * target_std['canSpeed'] ** 2\n",
    "                    validation_metrics['angle_loss'].setdefault(str(epoch), []).append(mse1)\n",
    "                    validation_metrics['speed_loss'].setdefault(str(epoch), []).append(mse2)\n",
    "            print('angle_loss: %.2f speed_loss: %.2f' % (np.mean(validation_metrics['angle_loss'][str(epoch)]), \n",
    "                  np.mean(validation_metrics['speed_loss'][str(epoch)])))\n",
    "            print('='*10 + 'validation finished' + '='*10)\n",
    "    return train_metrics, validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_path does not exist!\n",
      "==========start 1 epoch==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691469ca2f124747953a1916bfbc9dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2225), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-deedf953b9d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m result = train_nn(train_loader, validation_loader, model, optimizer, criterion, epochs=5, \n\u001b[0;32m---> 15\u001b[0;31m                   validation=True, load_path=LOAD_PATH, save_path=SAVE_PATH, print_freq=200)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-f3f35b3deeed>\u001b[0m in \u001b[0;36mtrain_nn\u001b[0;34m(train_loader, validation_loader, model, optimizer, criterion, epochs, validation, load_path, save_path, print_freq)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'canSpeed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'canSpeed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NOW = datetime.now().strftime(\"%m-%d-%H:%M\")\n",
    "MODEL_NAME = 'non_local'\n",
    "\n",
    "if not os.path.isdir(os.path.join('../output', MODEL_NAME)):\n",
    "    os.mkdir(os.path.join('../output', MODEL_NAME))\n",
    "\n",
    "LOAD_PATH = os.path.join('../output', MODEL_NAME, '.pth')\n",
    "SAVE_PATH = os.path.join('../output', MODEL_NAME, NOW + '_' + MODEL_NAME + '.pth')\n",
    "\n",
    "model = NonLocalModel()\n",
    "criterion =nn.MSELoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "\n",
    "result = train_nn(train_loader, validation_loader, model, optimizer, criterion, epochs=5, \n",
    "                  validation=True, load_path=LOAD_PATH, save_path=SAVE_PATH, print_freq=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmytfALkchq6"
   },
   "source": [
    "## Creating a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uL0kP0lYchq7"
   },
   "outputs": [],
   "source": [
    "def add_results(results, output):\n",
    "    steering = np.squeeze(output['canSteering'].cpu().data.numpy())\n",
    "    speed = np.squeeze(output['canSpeed'].cpu().data.numpy())\n",
    "    if normalize_targets:\n",
    "        steering = (steering*target_std['canSteering'])+target_mean['canSteering']\n",
    "        speed = (speed*target_std['canSpeed'])+target_mean['canSpeed']\n",
    "    if np.isscalar(steering):\n",
    "        steering = [steering]\n",
    "    if np.isscalar(speed):\n",
    "        speed = [speed]\n",
    "    results['canSteering'].extend(steering)\n",
    "    results['canSpeed'].extend(speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sf28f1B6chq-"
   },
   "source": [
    "We use pandas to create a submission file which is simply a 2-column csv with a canSteering and canSpeed prediction for each row in the **drive360_test.csv** a total of 305437 rows/predictions not including the header. See the **sample_submission.csv** file as an example.\n",
    "\n",
    "IMPORTANT: for the test phase indices will start 10s (100 samples) into each chapter this is to allow challenge participants to experiment with different temporal settings of data input. If challenge participants have a greater temporal length than 10s for each training sample, then they must write a custom function here. Please check out the **dataset.py** file for additional explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogBR8mhFchq_"
   },
   "outputs": [],
   "source": [
    "results = {'canSteering': [],\n",
    "           'canSpeed': []}\n",
    "\n",
    "model = SomeDrivingModel()\n",
    "model.cuda()\n",
    "MODEL_NAME = 'baseline'\n",
    "LOAD_PATH = os.path.join('../output', MODEL_NAME, '10-07-04:26_baseline.pth')\n",
    "checkpoint = torch.load(LOAD_PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tqdm_notebook(test_loader)):\n",
    "        prediction = model(data)\n",
    "        add_results(results, prediction)\n",
    "\n",
    "df = pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## linear interpolation\n",
    "from scipy.interpolate import interp1d\n",
    "file = os.path.join('../output', MODEL_NAME, 'submission.csv')\n",
    "output = {\n",
    "    'canSteering': [],\n",
    "    'canSpeed': []\n",
    "}\n",
    "test_sample = pd.read_csv('../data/test_sample1.csv')\n",
    "curr_list = test_sample['chapter'].value_counts()-10\n",
    "test_full = pd.read_csv('../data/test_full.csv')\n",
    "target_list = test_full['chapter'].value_counts()-100\n",
    "k = 0\n",
    "for ch in test_sample['chapter'].unique():\n",
    "    curr_num = curr_list[ch]\n",
    "    target_num = target_list[ch]\n",
    "    x = list(range(100, 100+10*curr_num, 10))\n",
    "    x.insert(0, 0)\n",
    "    x.append(target_num-1)\n",
    "    newx = list(range(0, target_num))\n",
    "    y1, y2 = list(df.iloc[k:(k+curr_num),0]), list(df.iloc[k:(k+curr_num),1])\n",
    "    y1.insert(0, y1[0])\n",
    "    y1.append(y1[-1])\n",
    "    f1 = interp1d(x, y1, kind='linear')\n",
    "    output['canSteering'].extend(f1(newx))\n",
    "    y2.insert(0, y2[0])\n",
    "    y2.append(y2[-1])\n",
    "    f2 = interp1d(x, y2, kind='linear')\n",
    "    output['canSpeed'].extend(f2(newx))\n",
    "    k += curr_num\n",
    "    \n",
    "output_df = pd.DataFrame(output)\n",
    "print(len(output_df))\n",
    "output_df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "zxx_trybaseline.ipynb",
   "provenance": [
    {
     "file_id": "1QKQXTMXpTsQc4Pb4xCgBCzd_Kxa-5Q5g",
     "timestamp": 1569968743899
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
